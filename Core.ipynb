{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32aaac7c",
   "metadata": {},
   "source": [
    "# vLLM From Scratch\n",
    "Making my own version of vLLM for inference system architecture (scale, visualization, etc)\n",
    "\n",
    "## Goal\n",
    "1. Loads LLM\n",
    "2. Streams generated tokens over HTTP\n",
    "3. Tracks and visualize KV cache memory usage per requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd28259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio transformers accelerate accelerate fastapi uvicorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ab20bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install --quiet transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6537c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75c665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Use metal\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cpu\"  # Force CPU for simplicity\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7279d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device != \"cpu\" else torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1470e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249d8a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "889faacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the capital of Virginia? answer: Richmond\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the capital of Virginia? answer\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8258381",
   "metadata": {},
   "source": [
    "## KV Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7201cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement inference pipeline with KV cache tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351ccaf",
   "metadata": {},
   "source": [
    "## Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f89d0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement inference pipeline with batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dcb66",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "```bash\n",
    "uvicorn app:app --reload --port 8000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60683a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/generate\")\n",
    "def generate(prompt: str = \"Why is the sky blue?\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=30)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return StreamingResponse(iter([response]), media_type=\"text/plain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300f452",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cdfd684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated (MB): 2200.09856\n"
     ]
    }
   ],
   "source": [
    "if device == \"mps\":\n",
    "    print(\"Allocated (MB):\", torch.mps.current_allocated_memory() / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb21ed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.9.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.11.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.13.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.15.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.16.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.16.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.17.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.17.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.18.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.18.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.19.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.19.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.20.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.20.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.21.self_attn.k_proj.weight: torch.Size([256, 2048])\n",
      "model.layers.21.self_attn.v_proj.weight: torch.Size([256, 2048])\n",
      "Total key/value projection weights: 44\n"
     ]
    }
   ],
   "source": [
    "# Print shape of key/value projection weights\n",
    "count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if \"k_proj\" in name or \"v_proj\" in name:\n",
    "        count += 1\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "print(f\"Total key/value projection weights: {count}\")\n",
    "\n",
    "# KV cache of each layer of the model, each layer has a key and value projection:\n",
    "# 2048D - input size, input to the attention layer. gets projected to 256D Query, 256D Key, 256D Value. The input is the token embedding from the previous layer, ex: [The], [sky], [is], [blue], where each token ([]) is a 2048D vector.\n",
    "# Each layer projects the 2048D input to 256D to the query, key, and value. These are then used to compute how much attention each token should pay to every other token in the sequence.\n",
    "# 256D - size of the Q/K/V\n",
    "# 2048D - size of the output, which is the same as the input size to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0b40e",
   "metadata": {},
   "source": [
    "# Inference Pipelines and Scheduling\n",
    "\n",
    "## Continuous Batching\n",
    " Continuous batching, widely used in modern inference systems, boosts compute throughput by prioritizing prefill and batching decode stages to enhance throughput  [18, 19]. As shown in Fig. 2(b), Requests 2 and 3 preempt Request 1’s decode, and all three decode together after their prefill, improving throughput over static batching (Fig. 2(a)).\n",
    "\n",
    "## Chunked Batching\n",
    " Chunked batching improves latency-throughput balance by splitting long sequences into smaller, fixed-size chunks. As shown in Fig. 2(c), this allows prefill (e.g., Request 2) to run alongside decode (e.g., Request 1), avoiding the stalls seen in prefill-prioritized strategies like continuous batching.\n",
    "\n",
    "## Disaggregated Batching \n",
    "Disaggregated serving decouples prefill and decode stages by assigning them to independently scaled hardware instances, enabling flexible resource allocation for heterogeneous workloads. For example in Fig. 2(d), Request 1 begins decoding while Requests 2 and 3 are still in prefill, due to this separation. Decode stages are then batched as resources become available, improving throughput. But it comes with the cost of KV cache transfer.\n",
    "\n",
    "We define two disaggregation types: Global, as in Splitwise [17], uses a shared GPU pool without locality constraints, offering better load balancing; and Local, which restricts requests to fixed, physically co-located GPU groups, reducing KV cache transfer overhead. By default, we use global disaggregated batching unless otherwise noted.\n",
    "\n",
    "from https://arxiv.org/html/2504.09775v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a356256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4359bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
